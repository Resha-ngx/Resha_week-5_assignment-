{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJunazJxcHPV",
        "outputId": "272cbb87-f179-49c2-c5bd-8e31ae2d7760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Land Rover Defender V8\n",
            "Document: Land Rover Defender V8 .txt, Score: 1.1968\n",
            "Document: Range Rover Sport SVR .txt, Score: 0.0184\n",
            "Document: Dodge Challenger SRT Hellcat.txt, Score: 0.0049\n",
            "Document: Jeep Grand Cherokee Trackhawk .txt, Score: 0.0049\n",
            "Document: Ford Mustang Shelby GT500 .txt, Score: 0.0048\n",
            "Document: Porsche Cayenne Turbo .txt, Score: 0.0047\n",
            "Document: Audi RS Q8 .txt, Score: 0.0046\n",
            "Document: Mercedes-AMG GLE 63 S .txt, Score: 0.0046\n",
            "Document: Mercedes-AMG GT .txt, Score: 0.0045\n",
            "Document: Chevrolet Corvette Z06 .txt, Score: 0.0045\n",
            "Document: BMW M5 .txt, Score: 0.0044\n",
            "Document: Jaguar F-PACE SVR .txt, Score: 0.0043\n",
            "Document: Lexus LX 600 .txt, Score: 0.0026\n",
            "Document: Tesla Model S Plaid .txt, Score: 0.0023\n",
            "Document: Volkswagen Touareg R .txt, Score: 0.0023\n",
            "Document: Acura MDX .txt, Score: 0.0022\n",
            "Document: Ford Explorer .txt, Score: 0.0022\n",
            "Document: Porsche 911 Turbo S .txt, Score: 0.0022\n",
            "Document: Chrysler Pacifica .txt, Score: 0.0021\n",
            "Document: Volvo XC90 .txt, Score: 0.0021\n",
            "\n",
            "Query: Porsche Cayenne Turbo\n",
            "Document: Porsche Cayenne Turbo .txt, Score: 2.1194\n",
            "Document: Porsche 911 Turbo S .txt, Score: 0.6507\n",
            "Document: Mercedes-AMG GT .txt, Score: 0.0385\n",
            "Document: Lexus LX 600 .txt, Score: 0.0216\n",
            "Document: Dodge Challenger SRT Hellcat.txt, Score: 0.0205\n",
            "Document: Jeep Grand Cherokee Trackhawk .txt, Score: 0.0204\n",
            "Document: Ford Mustang Shelby GT500 .txt, Score: 0.0200\n",
            "Document: Land Rover Defender V8 .txt, Score: 0.0199\n",
            "Document: Audi RS Q8 .txt, Score: 0.0196\n",
            "Document: Range Rover Sport SVR .txt, Score: 0.0194\n",
            "Document: Mercedes-AMG GLE 63 S .txt, Score: 0.0194\n",
            "Document: Tesla Model S Plaid .txt, Score: 0.0193\n",
            "Document: Chevrolet Corvette Z06 .txt, Score: 0.0192\n",
            "Document: Volkswagen Touareg R .txt, Score: 0.0192\n",
            "Document: Acura MDX .txt, Score: 0.0189\n",
            "Document: BMW M5 .txt, Score: 0.0188\n",
            "Document: Ford Explorer .txt, Score: 0.0188\n",
            "Document: Jaguar F-PACE SVR .txt, Score: 0.0185\n",
            "Document: Chrysler Pacifica .txt, Score: 0.0184\n",
            "Document: Volvo XC90 .txt, Score: 0.0181\n",
            "\n",
            "Query: Range Rover Sport SVR\n",
            "Document: Range Rover Sport SVR .txt, Score: 1.6033\n",
            "Document: Jaguar F-PACE SVR .txt, Score: 0.0269\n",
            "Document: Land Rover Defender V8 .txt, Score: 0.0089\n",
            "Document: Tesla Model S Plaid .txt, Score: 0.0057\n",
            "Document: Volkswagen Touareg R .txt, Score: 0.0056\n",
            "Document: Ford Explorer .txt, Score: 0.0055\n",
            "Document: Volvo XC90 .txt, Score: 0.0052\n",
            "Document: Lexus LX 600 .txt, Score: 0.0033\n",
            "Document: Dodge Challenger SRT Hellcat.txt, Score: 0.0031\n",
            "Document: Jeep Grand Cherokee Trackhawk .txt, Score: 0.0031\n",
            "Document: Ford Mustang Shelby GT500 .txt, Score: 0.0030\n",
            "Document: Porsche Cayenne Turbo .txt, Score: 0.0029\n",
            "Document: Audi RS Q8 .txt, Score: 0.0029\n",
            "Document: Mercedes-AMG GLE 63 S .txt, Score: 0.0029\n",
            "Document: Mercedes-AMG GT .txt, Score: 0.0028\n",
            "Document: Chevrolet Corvette Z06 .txt, Score: 0.0028\n",
            "Document: Acura MDX .txt, Score: 0.0027\n",
            "Document: BMW M5 .txt, Score: 0.0027\n",
            "Document: Porsche 911 Turbo S .txt, Score: 0.0027\n",
            "Document: Chrysler Pacifica .txt, Score: 0.0027\n",
            "\n",
            "Query: Mercedes-AMG GT\n",
            "Document: Mercedes-AMG GT .txt, Score: 6.4769\n",
            "Document: Mercedes-AMG GLE 63 S .txt, Score: 0.3106\n",
            "Document: Lexus LX 600 .txt, Score: 0.0288\n",
            "Document: Dodge Challenger SRT Hellcat.txt, Score: 0.0273\n",
            "Document: Jeep Grand Cherokee Trackhawk .txt, Score: 0.0273\n",
            "Document: Ford Mustang Shelby GT500 .txt, Score: 0.0267\n",
            "Document: Land Rover Defender V8 .txt, Score: 0.0266\n",
            "Document: Porsche Cayenne Turbo .txt, Score: 0.0262\n",
            "Document: Audi RS Q8 .txt, Score: 0.0261\n",
            "Document: Range Rover Sport SVR .txt, Score: 0.0259\n",
            "Document: Tesla Model S Plaid .txt, Score: 0.0257\n",
            "Document: Chevrolet Corvette Z06 .txt, Score: 0.0256\n",
            "Document: Volkswagen Touareg R .txt, Score: 0.0256\n",
            "Document: Acura MDX .txt, Score: 0.0252\n",
            "Document: BMW M5 .txt, Score: 0.0251\n",
            "Document: Ford Explorer .txt, Score: 0.0250\n",
            "Document: Porsche 911 Turbo S .txt, Score: 0.0248\n",
            "Document: Jaguar F-PACE SVR .txt, Score: 0.0248\n",
            "Document: Chrysler Pacifica .txt, Score: 0.0245\n",
            "Document: Volvo XC90 .txt, Score: 0.0241\n",
            "\n",
            "Query: Porsche 911 Turbo S\n",
            "Document: Porsche 911 Turbo S .txt, Score: 4.8633\n",
            "Document: Porsche Cayenne Turbo .txt, Score: 0.0549\n",
            "Document: Mercedes-AMG GLE 63 S .txt, Score: 0.0210\n",
            "Document: Tesla Model S Plaid .txt, Score: 0.0178\n",
            "Document: Mercedes-AMG GT .txt, Score: 0.0059\n",
            "Document: Acura MDX .txt, Score: 0.0058\n",
            "Document: Lexus LX 600 .txt, Score: 0.0035\n",
            "Document: Dodge Challenger SRT Hellcat.txt, Score: 0.0032\n",
            "Document: Jeep Grand Cherokee Trackhawk .txt, Score: 0.0032\n",
            "Document: Ford Mustang Shelby GT500 .txt, Score: 0.0031\n",
            "Document: Land Rover Defender V8 .txt, Score: 0.0031\n",
            "Document: Audi RS Q8 .txt, Score: 0.0030\n",
            "Document: Range Rover Sport SVR .txt, Score: 0.0030\n",
            "Document: Chevrolet Corvette Z06 .txt, Score: 0.0030\n",
            "Document: Volkswagen Touareg R .txt, Score: 0.0030\n",
            "Document: BMW M5 .txt, Score: 0.0029\n",
            "Document: Ford Explorer .txt, Score: 0.0029\n",
            "Document: Jaguar F-PACE SVR .txt, Score: 0.0028\n",
            "Document: Chrysler Pacifica .txt, Score: 0.0028\n",
            "Document: Volvo XC90 .txt, Score: 0.0027\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from math import log\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess(text):\n",
        "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "# Load documents\n",
        "\n",
        "def load_documents(folder_path):\n",
        "    docs = {}\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='latin-1') as file: # Changed encoding to 'latin-1'\n",
        "                docs[filename] = preprocess(file.read())\n",
        "    return docs\n",
        "# Load queries\n",
        "def load_queries(query_file_path):\n",
        "    with open(query_file_path, 'r') as file:\n",
        "        return [line.strip() for line in file.readlines()]\n",
        "\n",
        "# Compute term frequencies and document frequencies\n",
        "def compute_statistics(docs):\n",
        "    doc_count = len(docs)\n",
        "    term_doc_freq = defaultdict(int)\n",
        "    term_freq = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for doc_id, words in docs.items():\n",
        "        word_set = set(words)\n",
        "        for word in words:\n",
        "            term_freq[doc_id][word] += 1\n",
        "        for word in word_set:\n",
        "            term_doc_freq[word] += 1\n",
        "\n",
        "    return term_freq, term_doc_freq, doc_count\n",
        "\n",
        "# Compute relevance probabilities using BIM\n",
        "def compute_relevance_prob(query, term_freq, term_doc_freq, doc_count):\n",
        "    scores = {}\n",
        "    for doc_id in term_freq:\n",
        "        score = 1.0\n",
        "        for term in query:\n",
        "            tf = term_freq[doc_id].get(term, 0)\n",
        "            df = term_doc_freq.get(term, 0)\n",
        "            p_term_given_relevant = (tf + 1) / (sum(term_freq[doc_id].values()) + len(term_doc_freq))\n",
        "            p_term_given_not_relevant = (df + 1) / (doc_count - df + len(term_doc_freq))\n",
        "            score *= (p_term_given_relevant / p_term_given_not_relevant)\n",
        "        scores[doc_id] = score\n",
        "    return scores\n",
        "\n",
        "# Main retrieval function\n",
        "def retrieve_documents(folder_path, query_file_path):\n",
        "    docs = load_documents(folder_path)\n",
        "    queries = load_queries(query_file_path)\n",
        "\n",
        "    term_freq, term_doc_freq, doc_count = compute_statistics(docs)\n",
        "\n",
        "    for query in queries:\n",
        "        query_terms = preprocess(query)\n",
        "        scores = compute_relevance_prob(query_terms, term_freq, term_doc_freq, doc_count)\n",
        "        ranked_docs = sorted(scores.items(), key=lambda item: item[1], reverse=True)\n",
        "        print(f\"Query: {query}\")\n",
        "        for doc_id, score in ranked_docs:\n",
        "            print(f\"Document: {doc_id}, Score: {score:.4f}\")\n",
        "        print()\n",
        "\n",
        "# Example usage\n",
        "folder_path = '/content/drive/MyDrive/Tech400_final_project'\n",
        "query_file_path = '/content/drive/MyDrive/Query/query.txt'\n",
        "retrieve_documents(folder_path, query_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "import random\n",
        "\n",
        "# Function to assign random relevance scores\n",
        "def assign_random_relevance(queries, documents, relevance_scale=(0, 1)):\n",
        "    relevance_scores = {}\n",
        "\n",
        "    for query in queries:\n",
        "        relevance_scores[query] = {}  # Use the query string directly, no tuple or list\n",
        "        for doc in documents:\n",
        "            # Assign a random relevance score between relevance_scale (0 and 1 by default)\n",
        "            relevance_scores[query][doc] = random.randint(relevance_scale[0], relevance_scale[1])\n",
        "\n",
        "    return relevance_scores\n",
        "\n",
        "# Function to save relevance scores to a file\n",
        "def save_relevance_scores_to_file(relevance_scores, output_file):\n",
        "    with open(output_file, 'w') as f:\n",
        "        for query, doc_scores in relevance_scores.items():\n",
        "            for doc, score in doc_scores.items():\n",
        "                f.write(f\"{query},{doc},{score}\\n\")  # Write in the format query,document,score\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "folder_path = '/content/drive/MyDrive/Tech400_final_project'\n",
        "query_file_path = '/content/drive/MyDrive/Query/query.txt'\n",
        "# Load documents and queries\n",
        "documents = load_documents(folder_path)  # This returns a dictionary of doc_id -> content\n",
        "queries = load_queries(query_file_path)  # This returns a list of queries\n",
        "\n",
        "# Randomly assign relevance scores (0 for irrelevant, 1 for relevant)\n",
        "random_relevance_scores = assign_random_relevance(queries, documents.keys())\n",
        "\n",
        "# Save the relevance scores to query_relevance_score.txt\n",
        "output_file = '/content/drive/MyDrive/Query/query_score.txt'\n",
        "save_relevance_scores_to_file(random_relevance_scores, output_file)\n",
        "\n",
        "print(f\"Relevance scores saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsekclpKcoUo",
        "outputId": "f02673eb-1f75-4657-eba2-4a4a53323545"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevance scores saved to /content/drive/MyDrive/Query/query_score.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "06wQ4om-cwGw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}